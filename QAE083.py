"""
QAE083: æ··åˆç»å…¸-é‡å­ç¼–ç è§£ç ç¥ç»ç½‘ç»œ (ä½¿ç”¨CsiNetç¼–ç å™¨ï¼Œencoded_dim=256)

ç½‘ç»œæ¶æ„ï¼š
1. ç»å…¸ç¼–ç å™¨ï¼šä½¿ç”¨CsiNetå·ç§¯ç¼–ç å™¨å°†(2,32,32)å›¾åƒå‹ç¼©åˆ°256ç»´
2. é‡å­æ€æ˜ å°„ï¼šå°†256ç»´ç»å…¸å‘é‡æ˜ å°„ä¸ºé‡å­æ€ï¼ˆå¹…åº¦åµŒå…¥ï¼‰
3. é‡å­è§£ç å™¨ï¼šä½¿ç”¨å‚æ•°åŒ–é‡å­çº¿è·¯è§£ç é‡å­æ€ï¼Œæ¢å¤åˆ°2048ç»´
4. ç»å…¸è§£ç å™¨ï¼šå°†2048ç»´å‘é‡é‡å¡‘ä¸º(2,32,32)å›¾åƒæ ¼å¼

æ•°æ®æµï¼š
è¾“å…¥(2,32,32) -> CsiNetç¼–ç å™¨ -> 256ç»´ -> é‡å­æ€ -> é‡å­è§£ç å™¨ -> 2048ç»´ -> (2,32,32)
"""

import os
import time
import numpy as np
import torch
import torch.nn as nn
import pennylane as qml
import csv
import scipy.io as sio

# Reproducibility
torch.manual_seed(42)
np.random.seed(42)

# åˆ›å»ºè¾“å‡ºç›®å½•
OUTPUT_DIR = "QAE083"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# å›¾åƒå‚æ•° (åŒ¹é…CsiNet)
img_height = 32
img_width = 32
img_channels = 2
img_total = img_height * img_width * img_channels  # 2048
encoded_dim = 256  # å‹ç¼©ç‡1/8

def load_csinet_data():
    """åŠ è½½CsiNetæ ¼å¼çš„.matæ•°æ®æ–‡ä»¶å¹¶reshapeä¸ºå›¾åƒæ ¼å¼"""
    data_paths = {
        'train': "/Users/luxian/DataSpace/csinet/data/DATA_Htrainin.mat",
        'val': "/Users/luxian/DataSpace/csinet/data/DATA_Hvalin.mat",
        'test': "/Users/luxian/DataSpace/csinet/data/DATA_Htestin.mat"
    }
    
    print("æ­£åœ¨åŠ è½½CsiNetæ•°æ®é›†...")
    datasets = {}
    
    for key, path in data_paths.items():
        try:
            mat_data = sio.loadmat(path)
            x = mat_data['HT'].astype('float32')
            # å½’ä¸€åŒ–åˆ°[0,1]
            x = (x - x.min()) / (x.max() - x.min())
            # reshapeä¸ºå›¾åƒæ ¼å¼
            x = np.reshape(x, (len(x), img_channels, img_height, img_width))
            datasets[key] = x
            print(f"{key}æ•°æ®åŠ è½½æˆåŠŸ: {x.shape}")
        except Exception as e:
            raise FileNotFoundError(f"æ— æ³•åŠ è½½{key}æ•°æ® {path}: {e}")
    
    print(f"æ•°æ®èŒƒå›´: [{datasets['train'].min():.4f}, {datasets['train'].max():.4f}]")
    return datasets['train'], datasets['val'], datasets['test']

# ============================================================================
# 1. CsiNetç¼–ç å™¨ï¼ˆåŸºäºKeraså®ç°è½¬æ¢ä¸ºPyTorchï¼‰
# ============================================================================

class CsiNetEncoder(nn.Module):
    """CsiNetç¼–ç å™¨ï¼šå°†(2,32,32)å›¾åƒå‹ç¼©åˆ°256ç»´å‘é‡"""
    
    def __init__(self, encoded_dim=256):  # ä¿®æ”¹ï¼šé»˜è®¤å‚æ•°æ”¹ä¸º256
        super(CsiNetEncoder, self).__init__()
        self.encoded_dim = encoded_dim
        
        # ç¬¬ä¸€å±‚å·ç§¯
        self.conv1 = nn.Conv2d(2, 2, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(2)
        self.relu1 = nn.LeakyReLU(0.3)
        
        # å…¨è¿æ¥å±‚è¿›è¡Œå‹ç¼©
        self.flatten = nn.Flatten()
        self.dense_encoded = nn.Linear(img_total, encoded_dim)
        
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        x: (batch_size, 2, 32, 32)
        """
        # ç¬¬ä¸€å±‚å·ç§¯ + BN + LeakyReLU
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu1(x)
        
        # flattenå¹¶å‹ç¼©åˆ°ç¼–ç ç»´åº¦
        x = self.flatten(x)
        encoded = self.dense_encoded(x)
        
        # ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„å®æ•°å‘é‡ï¼ˆç”¨äºé‡å­æŒ¯å¹…åµŒå…¥ï¼‰
        encoded = torch.clamp(encoded, min=1e-7, max=1e7)  # é˜²æ­¢æç«¯å€¼
        encoded = torch.nan_to_num(encoded, nan=0.0, posinf=1.0, neginf=0.0)  # å¤„ç†NaNå’Œæ— ç©·å¤§
        
        return encoded

# ============================================================================
# 2. é‡å­æ€æ˜ å°„å’Œé‡å­è§£ç å™¨
# ============================================================================

def normalize_for_amplitude_embedding(vec):
    """å½’ä¸€åŒ–å‘é‡ç”¨äºå¹…åº¦åµŒå…¥ï¼ˆä¿æŒæ¢¯åº¦æµï¼‰"""
    # ç¡®ä¿å‘é‡æ˜¯å®æ•°ä¸”æœ‰æ•ˆ
    if isinstance(vec, torch.Tensor):
        vec = torch.nan_to_num(vec, nan=0.0, posinf=1.0, neginf=0.0)
        vec = torch.clamp(vec, min=0.0, max=1e7)  # ç¡®ä¿éè´Ÿ
        norm = torch.norm(vec, p=2)
    else:
        vec = np.nan_to_num(vec, nan=0.0, posinf=1.0, neginf=0.0)
        vec = np.clip(vec, 0.0, 1e7)
        norm = np.linalg.norm(vec, ord=2)
    
    # å¦‚æœèŒƒæ•°å¤ªå°ï¼Œè¿”å›å‡åŒ€åˆ†å¸ƒ
    if norm < 1e-10:
        if isinstance(vec, torch.Tensor):
            return torch.ones_like(vec) / torch.sqrt(torch.tensor(float(len(vec))))
        else:
            return np.ones_like(vec) / np.sqrt(len(vec))
    
    return vec / norm

def pad_to_qubits(vec, n_qubits):
    """å¡«å……å‘é‡åˆ°2^n_qubitsç»´åº¦"""
    target_len = 2 ** n_qubits
    if isinstance(vec, torch.Tensor):
        # ç¡®ä¿å‘é‡æ˜¯æœ‰æ•ˆçš„
        vec = torch.nan_to_num(vec, nan=0.0, posinf=1.0, neginf=0.0)
        vec = torch.clamp(vec, min=0.0, max=1e7)
        if len(vec) < target_len:
            return torch.nn.functional.pad(vec, (0, target_len - len(vec)))
        return vec[:target_len]
    else:
        vec = np.nan_to_num(vec, nan=0.0, posinf=1.0, neginf=0.0)
        vec = np.clip(vec, 0.0, 1e7)
        if len(vec) < target_len:
            return np.pad(vec, (0, target_len - len(vec)))
        return vec[:target_len]

# ============================================================================
# 2. é‡å­æ€æ˜ å°„å’Œé‡å­è§£ç å™¨
# ============================================================================

# Quantum device
# ä½¿ç”¨11ä¸ªé‡å­æ¯”ç‰¹ï¼š8ä¸ªç”¨äºç¼–ç 256ç»´æ•°æ®ï¼Œ3ä¸ªç”¨äºansatzæ“ä½œï¼Œå…¨éƒ¨11ä¸ªç”¨äºæµ‹é‡å¾—åˆ°2048ç»´è¾“å‡º
DEV = qml.device("lightning.qubit", wires=11)  

@qml.qnode(DEV, interface="torch")
def quantum_decoder_circuit(encoded_vec, dec_params):
    """
    é‡å­è‡ªç¼–ç å™¨è§£ç å™¨ç”µè·¯
    
    Args:
        encoded_vec: ç»å…¸ç¼–ç å™¨è¾“å‡ºçš„256ç»´å‘é‡
        dec_params: é‡å­è§£ç å™¨å‚æ•°
        
    Returns:
        2048ç»´æ¦‚ç‡åˆ†å¸ƒï¼ˆå¯¹åº”11ä¸ªé‡å­æ¯”ç‰¹åœ¨è®¡ç®—åŸºä¸‹çš„æµ‹é‡æ¦‚ç‡ï¼‰
    """
    # 1. å°†ç»å…¸ç¼–ç å‘é‡åµŒå…¥ä¸ºé‡å­æ€ï¼ˆä½¿ç”¨å‰8ä¸ªé‡å­æ¯”ç‰¹ï¼‰
    encoded_padded = pad_to_qubits(encoded_vec, 8)
    encoded_normalized = normalize_for_amplitude_embedding(encoded_padded)
    
    # é¢å¤–ç¡®ä¿å½’ä¸€åŒ–ï¼ˆåŒé‡ä¿é™©ï¼‰
    encoded_normalized = encoded_normalized / (torch.norm(encoded_normalized, p=2) + 1e-10)
    
    # åœ¨å‰8ä¸ªé‡å­æ¯”ç‰¹ä¸Šè¿›è¡ŒæŒ¯å¹…ç¼–ç 
    qml.AmplitudeEmbedding(encoded_normalized, wires=range(8), 
                          pad_with=0.0, normalize=True)
    
    # å3ä¸ªé‡å­æ¯”ç‰¹åˆå§‹åŒ–ä¸º|0>æ€ï¼ˆé»˜è®¤å·²æ˜¯|0>ï¼Œæ— éœ€é¢å¤–æ“ä½œï¼‰
    
    # 2. åº”ç”¨å‚æ•°åŒ–é‡å­è§£ç å±‚ï¼ˆä½œç”¨äºå…¨éƒ¨11ä¸ªé‡å­æ¯”ç‰¹ï¼‰
    qml.StronglyEntanglingLayers(weights=dec_params, wires=range(11))
    
    # 3. è®¡ç®—åŸºæµ‹é‡ï¼Œè¿”å›2048ä¸ªåŸºæ€çš„æ¦‚ç‡åˆ†å¸ƒ
    return qml.probs(wires=range(11))

# ============================================================================
# 3. å®Œæ•´çš„æ··åˆç½‘ç»œ

class HybridCsiNetQuantumAutoencoder(nn.Module):
    """
    å®Œæ•´çš„æ··åˆCsiNet-é‡å­è‡ªç¼–ç å™¨
    
    æµç¨‹ï¼š
    1. CsiNetç¼–ç å™¨å‹ç¼©å›¾åƒåˆ°256ç»´
    2. é‡å­æ€åµŒå…¥ï¼ˆ8ä¸ªé‡å­æ¯”ç‰¹ï¼‰å’Œé‡å­è§£ç å™¨å˜æ¢ï¼ˆ11ä¸ªé‡å­æ¯”ç‰¹ï¼‰
    3. è®¡ç®—åŸºæµ‹é‡å¾—åˆ°2048ç»´æ¦‚ç‡åˆ†å¸ƒï¼ˆç›´æ¥ä½œä¸ºè¾“å‡ºï¼‰
    """
    def __init__(self, csinet_encoder, dec_params):
        super(HybridCsiNetQuantumAutoencoder, self).__init__()
        self.csinet_encoder = csinet_encoder
        self.dec_params = dec_params
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        x: (batch_size, 2, 32, 32)
        è¿”å›: (batch_size, 2048) æ¦‚ç‡åˆ†å¸ƒ
        """
        batch_size = x.shape[0]
        
        # 1. CsiNetç¼–ç å™¨å¤„ç†
        encoded_batch = self.csinet_encoder(x)  # (batch_size, 256)
        
        # 2. é‡å­è§£ç ï¼ˆé€ä¸ªæ ·æœ¬å¤„ç†ï¼‰
        outputs = []
        for i in range(batch_size):
            # è·å–å•ä¸ªç¼–ç å‘é‡
            encoded_vec = encoded_batch[i]  # (256,)
            
            # é‡å­è§£ç ï¼ˆè¿”å›2048ä¸ªæ¦‚ç‡å€¼ï¼‰
            quantum_probs = quantum_decoder_circuit(encoded_vec, self.dec_params)  # (2048,)
            outputs.append(quantum_probs.unsqueeze(0))
        
        # åˆå¹¶æ‰€æœ‰æ ·æœ¬ï¼Œè¿”å›æ¦‚ç‡åˆ†å¸ƒ
        result = torch.cat(outputs, dim=0)  # (batch_size, 2048)
        
        return result

# ============================================================================
# 4. è®­ç»ƒå’Œæµ‹è¯•å‡½æ•°
# ============================================================================

def prepare_target_distribution(batch):
    """
    å°†åŸå§‹è¾“å…¥batchè½¬æ¢ä¸ºå½’ä¸€åŒ–çš„ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒ
    
    Args:
        batch: (batch_size, 2, 32, 32) åŸå§‹è¾“å…¥
    
    Returns:
        (batch_size, 2048) å½’ä¸€åŒ–çš„æ¦‚ç‡åˆ†å¸ƒ
    """
    batch_size = batch.shape[0]
    targets = []
    
    for i in range(batch_size):
        # å°†å›¾åƒå±•å¹³ä¸º2048ç»´å‘é‡
        vec = batch[i].view(-1)  # (2048,)
        
        # å¡«å……åˆ°2^11=2048ï¼ˆè¿™é‡Œå·²ç»æ˜¯2048ï¼Œæ— éœ€å¡«å……ï¼‰
        vec_padded = pad_to_qubits(vec, 11)
        
        # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        vec_normalized = normalize_for_amplitude_embedding(vec_padded)
        
        # ç›´æ¥ä½¿ç”¨å½’ä¸€åŒ–å‘é‡ä½œä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆæ•°æ®å·²é¢„å¤„ç†ä¸º[0,1]æ­£å®æ•°ï¼‰
        prob_dist = vec_normalized
        
        # å½’ä¸€åŒ–ç¡®ä¿å’Œä¸º1
        prob_dist = prob_dist / (prob_dist.sum() + 1e-10)
        
        targets.append(prob_dist.unsqueeze(0))
    
    return torch.cat(targets, dim=0)

def save_initial_parameters(csinet_encoder, dec_params):
    """ä¿å­˜åˆå§‹å‚æ•°"""
    torch.save(csinet_encoder.state_dict(), 
              f"{OUTPUT_DIR}/initial_csinet_encoder.pt")
    torch.save(dec_params, 
              f"{OUTPUT_DIR}/initial_quantum_decoder_weights.pt")
    print("åˆå§‹å‚æ•°å·²ä¿å­˜ï¼")

def compute_probability_loss(output_probs, target_probs, loss_type='cross_entropy'):
    """
    è®¡ç®—ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„æŸå¤±
    
    Args:
        output_probs: æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒ (batch_size, 2048)
        target_probs: ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒ (batch_size, 2048)
        loss_type: 'kl', 'mse', 'cross_entropy', 'jsd', 'hellinger' (é»˜è®¤äº¤å‰ç†µ)
    
    Returns:
        æŸå¤±å€¼
    """
    epsilon = 1e-10  # é¿å…log(0)å’Œé™¤é›¶
    
    if loss_type == 'mse':
        # æ¦‚ç‡åˆ†å¸ƒçš„å‡æ–¹è¯¯å·®
        return torch.mean((output_probs - target_probs) ** 2)
    
    elif loss_type == 'kl':
        # KLæ•£åº¦: KL(target || output)
        kl_div = target_probs * torch.log((target_probs + epsilon) / (output_probs + epsilon))
        return torch.mean(torch.sum(kl_div, dim=1))
    
    elif loss_type == 'cross_entropy':
        # äº¤å‰ç†µ: -sum(target * log(output))
        ce_loss = -target_probs * torch.log(output_probs + epsilon)
        return torch.mean(torch.sum(ce_loss, dim=1))
    
    elif loss_type == 'jsd':
        # Jensen-Shannonæ•£åº¦: 1/2 * KL(target||M) + 1/2 * KL(output||M)
        # å…¶ä¸­ M = (target + output) / 2
        M = (target_probs + output_probs) / 2
        kl_target_M = target_probs * torch.log((target_probs + epsilon) / (M + epsilon))
        kl_output_M = output_probs * torch.log((output_probs + epsilon) / (M + epsilon))
        jsd = 0.5 * torch.sum(kl_target_M + kl_output_M, dim=1)
        return torch.mean(jsd)
    
    elif loss_type == 'hellinger':
        # Hellingerè·ç¦»: (1/âˆš2) * ||âˆštarget - âˆšoutput||_2
        sqrt_target = torch.sqrt(target_probs + epsilon)
        sqrt_output = torch.sqrt(output_probs + epsilon)
        hellinger_dist = torch.norm(sqrt_target - sqrt_output, p=2, dim=1) / torch.sqrt(torch.tensor(2.0))
        return torch.mean(hellinger_dist)
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±ç±»å‹: {loss_type}. æ”¯æŒ: 'mse', 'kl', 'cross_entropy', 'jsd', 'hellinger'")

def validate_model(model, val_data, val_samples=500, loss_type='cross_entropy'):
    """éªŒè¯æ¨¡å‹ï¼Œé»˜è®¤ä½¿ç”¨äº¤å‰ç†µ"""
    model.eval()
    try:
        subset = torch.from_numpy(val_data[:min(val_samples, len(val_data))]).float()
        with torch.no_grad():
            outputs = model(subset)  # (batch, 2048) æ¦‚ç‡åˆ†å¸ƒ
            targets = prepare_target_distribution(subset)  # (batch, 2048) ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒ
            loss = compute_probability_loss(outputs, targets, loss_type=loss_type)
        model.train()
        return float(loss)
    except Exception as e:
        print(f"éªŒè¯é”™è¯¯: {e}")
        model.train()
        return float("nan")

def format_time(seconds):
    """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        minutes = seconds // 60
        secs = seconds % 60
        return f"{int(minutes)}åˆ†{secs:.0f}ç§’"
    else:
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        return f"{int(hours)}å°æ—¶{int(minutes)}åˆ†"

def train_hybrid_model():
    """è®­ç»ƒæ··åˆCsiNet-é‡å­è‡ªç¼–ç å™¨"""
    try:
        print("\n" + "=" * 80)
        print("ğŸš€ å¼€å§‹QAE083æ··åˆé‡å­è‡ªç¼–ç å™¨è®­ç»ƒ")
        print("=" * 80)
        
        # åˆå§‹åŒ–ç»„ä»¶
        csinet_encoder = CsiNetEncoder(encoded_dim=256)
        print("ğŸ“‹ CsiNetç¼–ç å™¨ç»“æ„:")
        print(csinet_encoder)
        
        # åˆå§‹åŒ–é‡å­è§£ç å™¨å‚æ•°ï¼šä½¿ç”¨11ä¸ªé‡å­æ¯”ç‰¹è¿›è¡Œansatzå˜æ¢
        dec_shape = qml.StronglyEntanglingLayers.shape(n_layers=4, n_wires=11)
        dec_params = nn.Parameter(torch.rand(dec_shape) * 2 * 3.14159 - 3.14159)  # åˆå§‹åŒ–ä¸º[-Ï€, Ï€]èŒƒå›´
        print(f"\nâš›ï¸  é‡å­è§£ç å™¨é…ç½®:")
        print(f"  â€¢ å‚æ•°å½¢çŠ¶: {dec_shape}")
        print(f"  â€¢ é‡å­æ¯”ç‰¹: 11 (8ä¸ªç”¨äº256ç»´ç¼–ç ï¼Œ11ä¸ªç”¨äºansatzå’Œæµ‹é‡)")
        
        # ä¿å­˜åˆå§‹å‚æ•°
        save_initial_parameters(csinet_encoder, dec_params)
        
        # åˆ›å»ºæ··åˆæ¨¡å‹
        hybrid_model = HybridCsiNetQuantumAutoencoder(csinet_encoder, dec_params)
        print(f"\nğŸ¤– æ··åˆæ¨¡å‹åˆ›å»ºå®Œæˆ: CsiNetç¼–ç å™¨ + é‡å­è§£ç å™¨")
        
        # ä¼˜åŒ–å™¨
        quantum_optimizer = torch.optim.Adam([dec_params], lr=0.001)
        classical_optimizer = torch.optim.Adam(csinet_encoder.parameters(), lr=0.001)
        
        # è®­ç»ƒå‚æ•°
        n_epochs = 5  # æ¢å¤åˆ°5ä¸ªepoch
        batch_size = 10  # è°ƒæ•´ä¸º10ï¼Œæ¯ä¸ªbatchå¤„ç†10ä¸ªæ ·æœ¬
        n_samples = 500  # ä¿æŒ500ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªepochæœ‰50ä¸ªbatch (500/10=50)
        samples = torch.from_numpy(train_data[:n_samples]).float()
        
        # è®­ç»ƒå†å²
        training_history = {
            "epoch_losses": [],
            "val_mse": [],
            "batch_losses": [],
            "data_split_info": {
                "train_size": len(train_data),
                "val_size": len(val_data),
                "test_size": len(test_data),
                "actual_train_used": n_samples,
            },
            "network_config": {
                "encoded_dim": 256,
                "quantum_encoding_qubits": 8,
                "quantum_ansatz_qubits": 11,
                "quantum_layers": 4,
                "output_dim": 2048,
                "compression_ratio": "1/8"
            }
        }
        
        # CSVæ–‡ä»¶è®°å½•
        csv_file = f"{OUTPUT_DIR}/hybrid_batch_losses.csv"
        with open(csv_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['epoch', 'batch', 'loss', 'dec_params_norm'])
        
        print(f"\nğŸ¯ è®­ç»ƒé…ç½®æ¦‚è§ˆ:")
        print(f"  â€¢ ç¼–ç ç»´åº¦: 256 (å‹ç¼©ç‡ 1/8)")
        print(f"  â€¢ é‡å­æ¯”ç‰¹: 11 (8ç¼–ç  + 3è¾…åŠ©)")
        print(f"  â€¢ é‡å­å±‚æ•°: 4")
        print(f"  â€¢ è¾“å‡ºç»´åº¦: 2048 (æ¦‚ç‡åˆ†å¸ƒ)")
        print(f"  â€¢ æ€»epochs: {n_epochs}")
        print(f"  â€¢ è®­ç»ƒæ ·æœ¬: {n_samples}")
        
        start_time = time.time()
        print(f"\nâ° è®­ç»ƒå¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        
        for epoch in range(n_epochs):
            hybrid_model.train()
            epoch_loss = 0.0
            batch_count = 0
            
            # éšæœºæ‰“ä¹±è®­ç»ƒæ•°æ®
            indices = torch.randperm(n_samples)
            samples_shuffled = samples[indices]
            
            epoch_start_time = time.time()
            batch_losses = []  # è®°å½•æœ¬è½®æ‰€æœ‰batchçš„æŸå¤±
            
            for i in range(0, n_samples, batch_size):
                batch = samples_shuffled[i:i + batch_size]
                actual_batch_size = batch.shape[0]
                
                if actual_batch_size < 1:  # ä¿®æ”¹æ¡ä»¶ï¼Œå…è®¸batch_size=1
                    continue
                    
                # æ¸…é›¶æ¢¯åº¦
                classical_optimizer.zero_grad()
                quantum_optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­ - å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
                outputs = hybrid_model(batch)  # (batch_size, 2048)
                
                # å‡†å¤‡ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒ
                targets = prepare_target_distribution(batch)  # (batch_size, 2048)
                
                # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„æŸå¤±ï¼ˆé»˜è®¤ä½¿ç”¨äº¤å‰ç†µï¼‰
                loss = compute_probability_loss(outputs, targets)
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # è®°å½•å‚æ•°èŒƒæ•°
                dec_params_norm = torch.norm(dec_params).item()
                
                # æ›´æ–°å‚æ•°
                classical_optimizer.step()
                quantum_optimizer.step()
                
                current_loss = loss.item()
                epoch_loss += current_loss * actual_batch_size
                batch_count += actual_batch_size
                batch_losses.append(current_loss)
                
                # è®°å½•batch loss
                training_history["batch_losses"].append({
                    "epoch": epoch,
                    "batch": i // batch_size,
                    "loss": float(current_loss),
                    "dec_params_norm": float(dec_params_norm)
                })
                
                # å†™å…¥CSV
                with open(csv_file, 'a', newline='') as f:
                    csv.writer(f).writerow([epoch, i // batch_size, current_loss, dec_params_norm])
                
                if (i // batch_size) % 10 == 0:
                    print(f"  Batch {(i//batch_size)+1:2d}/{(n_samples//batch_size):2d}: "
                          f"Loss = {current_loss:.8f}")
            
            if batch_count > 0:
                avg_epoch_loss = epoch_loss / batch_count
                epoch_time = time.time() - epoch_start_time
                val_mse = validate_model(hybrid_model, val_data, val_samples=200)
                
                training_history["epoch_losses"].append({"epoch": epoch, "avg_loss": float(avg_epoch_loss)})
                training_history["val_mse"].append({"epoch": epoch, "val_mse": float(val_mse)})
                
                # ä¿å­˜epochæƒé‡
                torch.save(csinet_encoder.state_dict(), 
                          f"{OUTPUT_DIR}/csinet_encoder_epoch_{epoch}.pt")
                torch.save(dec_params.clone().detach(), 
                          f"{OUTPUT_DIR}/quantum_decoder_epoch_{epoch}.pt")
                
                # ä¿å­˜ä¸­é—´è®­ç»ƒå†å²
                torch.save(training_history, 
                          f"{OUTPUT_DIR}/training_history_epoch_{epoch}.pt")
                
                # è¯¦ç»†çš„epochç»“æŸä¿¡æ¯æ‰“å°
                print("\n" + "=" * 80)
                print(f"ğŸ‰ EPOCH {epoch} è®­ç»ƒå®Œæˆ!")
                print("=" * 80)
                
                # è®­ç»ƒç»Ÿè®¡
                print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
                print(f"  â€¢ å¹³å‡è®­ç»ƒæŸå¤±: {avg_epoch_loss:.8f}")
                print(f"  â€¢ éªŒè¯é›†KLæ•£åº¦: {val_mse:.8f}")
                print(f"  â€¢ å¤„ç†æ ·æœ¬æ•°: {batch_count}")
                print(f"  â€¢ Epochè€—æ—¶: {format_time(epoch_time)}")
                print(f"  â€¢ å¹³å‡æ‰¹å¤„ç†æ—¶é—´: {epoch_time/batch_count:.4f}ç§’/æ ·æœ¬")
                
                # æŸå¤±è¯¦æƒ…
                print(f"\nğŸ“‰ æŸå¤±åˆ†æ:")
                print(f"  â€¢ æœ€å°batchæŸå¤±: {min(batch_losses):.8f}")
                print(f"  â€¢ æœ€å¤§batchæŸå¤±: {max(batch_losses):.8f}")
                print(f"  â€¢ æŸå¤±æ ‡å‡†å·®: {np.std(batch_losses):.8f}")
                print(f"  â€¢ æŸå¤±æ”¹å–„ç‡: {((batch_losses[0] - batch_losses[-1])/batch_losses[0]*100):.2f}%")
                
                # å‚æ•°çŠ¶æ€
                print(f"\nâš™ï¸  å‚æ•°çŠ¶æ€:")
                print(f"  â€¢ é‡å­å‚æ•°èŒƒæ•°: {dec_params_norm:.4f}")
                print(f"  â€¢ ç»å…¸ç¼–ç å™¨å‚æ•°æ•°: {sum(p.numel() for p in csinet_encoder.parameters()):,}")
                print(f"  â€¢ é‡å­è§£ç å™¨å‚æ•°æ•°: {dec_params.numel():,}")
                print(f"  â€¢ æ€»å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in csinet_encoder.parameters()) + dec_params.numel():,}")
                
                # ä¿å­˜ä¿¡æ¯
                print(f"\nğŸ’¾ ä¿å­˜çŠ¶æ€:")
                print(f"  â€¢ ç¼–ç å™¨æƒé‡: csinet_encoder_epoch_{epoch}.pt")
                print(f"  â€¢ é‡å­å‚æ•°: quantum_decoder_epoch_{epoch}.pt")
                print(f"  â€¢ è®­ç»ƒå†å²: training_history_epoch_{epoch}.pt")
                
                # è¿›åº¦æ¡
                progress = (epoch + 1) / n_epochs * 100
                bar_length = 30
                filled_length = int(bar_length * progress // 100)
                bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
                print(f"\nğŸ”„ æ€»ä½“è¿›åº¦: |{bar}| {progress:.1f}% ({epoch + 1}/{n_epochs})")
                print("=" * 80)
        
        total_time = time.time() - start_time
        print(f"\nğŸ† è®­ç»ƒåœ†æ»¡å®Œæˆ!")
        print("=" * 80)
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {format_time(total_time)}")
        print(f"ğŸ“… è®­ç»ƒç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        torch.save(csinet_encoder.state_dict(), 
                  f"{OUTPUT_DIR}/final_csinet_encoder.pt")
        torch.save(dec_params, 
                  f"{OUTPUT_DIR}/final_quantum_decoder_weights.pt")
        torch.save(training_history, 
                  f"{OUTPUT_DIR}/training_history.pt")
        
        print(f"\nğŸ’¾ æœ€ç»ˆæ¨¡å‹ä¿å­˜:")
        print(f"  â€¢ æœ€ç»ˆç¼–ç å™¨: final_csinet_encoder.pt")
        print(f"  â€¢ æœ€ç»ˆé‡å­å‚æ•°: final_quantum_decoder_weights.pt")
        print(f"  â€¢ å®Œæ•´è®­ç»ƒå†å²: training_history.pt")
        print("=" * 80)
        
        return hybrid_model, training_history
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_trained_model(model, test_data, test_samples=500):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print("\n" + "=" * 70)
    print("åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
    print("=" * 70)
    try:
        model.eval()
        subset = torch.from_numpy(test_data[:min(test_samples, len(test_data))]).float()
        
        with torch.no_grad():
            # æ¨¡å‹è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
            outputs = model(subset)  # (batch, 2048)
            # å‡†å¤‡ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒ
            targets = prepare_target_distribution(subset)  # (batch, 2048)
            # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„æŸå¤±ï¼ˆä½¿ç”¨é»˜è®¤äº¤å‰ç†µï¼‰
            prob_loss = compute_probability_loss(outputs, targets)
            
        print(f"æµ‹è¯•é›†äº¤å‰ç†µæŸå¤±ï¼ˆ{len(subset)} ä¸ªæ ·æœ¬ï¼‰: {prob_loss:.6f}")
        
        # è®¡ç®—å…¶ä»–æŸå¤±å‡½æ•°ä½œä¸ºå¯¹æ¯”æŒ‡æ ‡
        with torch.no_grad():
            mse_loss = compute_probability_loss(outputs, targets, loss_type='mse')
            kl_loss = compute_probability_loss(outputs, targets, loss_type='kl')
            jsd_loss = compute_probability_loss(outputs, targets, loss_type='jsd')
            hellinger_loss = compute_probability_loss(outputs, targets, loss_type='hellinger')
        
        print(f"æµ‹è¯•é›† MSEæŸå¤±: {mse_loss:.6f}")
        print(f"æµ‹è¯•é›† KLæ•£åº¦: {kl_loss:.6f}")
        print(f"æµ‹è¯•é›† JSDæ•£åº¦: {jsd_loss:.6f}")
        print(f"æµ‹è¯•é›† Hellingerè·ç¦»: {hellinger_loss:.6f}")
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        test_results = {
            "test_cross_entropy_loss": float(prob_loss),
            "test_mse_loss": float(mse_loss),
            "test_kl_divergence": float(kl_loss),
            "test_jsd_divergence": float(jsd_loss),
            "test_hellinger_distance": float(hellinger_loss),
            "n_samples": len(subset),
            "encoded_dim": 256,
            "loss_function_used": "cross_entropy"  # è®°å½•ä½¿ç”¨çš„æŸå¤±å‡½æ•°
        }
        torch.save(test_results, f"{OUTPUT_DIR}/test_results.pt")
        print("æµ‹è¯•ç»“æœå·²ä¿å­˜ï¼")
        
        return float(prob_loss)
    except Exception as e:
        print(f"æµ‹è¯•é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

# ============================================================================
# 5. ä¸»ç¨‹åº
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ”¬ QAE083: æ··åˆCsiNet-é‡å­ç¼–ç è§£ç ç¥ç»ç½‘ç»œ (encoded_dim=256)")
    print("=" * 80)
    
    # Data loading
    train_data, val_data, test_data = load_csinet_data()
    
    print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"  â€¢ è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_data):,}")
    print(f"  â€¢ éªŒè¯é›†æ ·æœ¬æ•°: {len(val_data):,}")
    print(f"  â€¢ æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_data):,}")
    print(f"  â€¢ è¾“å…¥å½¢çŠ¶: {train_data.shape[1:]}")
    print(f"  â€¢ æ•°æ®èŒƒå›´: [{train_data.min():.4f}, {train_data.max():.4f}]")
    
    print(f"\nğŸ—ï¸  ç½‘ç»œæ¶æ„ (encoded_dim=256):")
    print("  1. CsiNetç¼–ç å™¨: (2,32,32) â†’ 256ç»´ (å‹ç¼©ç‡1/8)")
    print("  2. é‡å­æ€åµŒå…¥: 256ç»´å‘é‡æ˜ å°„ä¸ºé‡å­æ€ (8é‡å­æ¯”ç‰¹æŒ¯å¹…ç¼–ç )")
    print("  3. é‡å­è§£ç å™¨: 11é‡å­æ¯”ç‰¹å‚æ•°åŒ–é‡å­çº¿è·¯ (StronglyEntanglingLayers)")
    print("  4. è®¡ç®—åŸºæµ‹é‡: ç›´æ¥å¾—åˆ°2048ç»´æ¦‚ç‡åˆ†å¸ƒ")
    print("  5. æŸå¤±å‡½æ•°: è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ vs è¾“å…¥å½’ä¸€åŒ–æ¦‚ç‡åˆ†å¸ƒçš„KLæ•£åº¦ (å¯é€‰: mse, cross_entropy, jsd, hellinger)")
    
    print("=" * 80)
    print("ğŸš€ å¼€å§‹è®­ç»ƒæµç¨‹...")
    
    # è®­ç»ƒæ¨¡å‹
    trained_model, history = train_hybrid_model()
    
    if trained_model is not None:
        # æµ‹è¯•æ¨¡å‹
        test_loss = test_trained_model(trained_model, test_data, test_samples=500)
        
        print("\n" + "=" * 70)
        print("è®­ç»ƒå’Œæµ‹è¯•å®Œæˆï¼")
        print("=" * 70)
        print(f"æ‰€æœ‰ç»“æœä¿å­˜åœ¨ç›®å½•: {OUTPUT_DIR}/")
        print(f"é…ç½®è¯¦æƒ…: æ¦‚ç‡åˆ†å¸ƒå¯¹æ¦‚ç‡åˆ†å¸ƒè®­ç»ƒ, 8æ¯”ç‰¹ç¼–ç +11æ¯”ç‰¹ansatz")
    else:
        print("\nè®­ç»ƒå¤±è´¥ï¼")