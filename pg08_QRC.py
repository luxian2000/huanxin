import pennylane as qml
import torch
import torch.nn as nn
import numpy as np
import time
import os

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
torch.manual_seed(42)
np.random.seed(42)

# æ•°æ®åŠ è½½
data_30 = np.load('CSI_channel_30km.npy')  # shape=(80000, 2560)

# æ•°æ®åˆ’åˆ†å‚æ•°
TOTAL_SAMPLES = 80000
TRAIN_RATIO = 0.70    # 70% è®­ç»ƒ
VAL_RATIO = 0.15      # 15% éªŒè¯  
TEST_RATIO = 0.15     # 15% æµ‹è¯•

# è®¡ç®—å„é›†åˆå¤§å°
train_size = int(TOTAL_SAMPLES * TRAIN_RATIO)
val_size = int(TOTAL_SAMPLES * VAL_RATIO)
test_size = TOTAL_SAMPLES - train_size - val_size

# åˆ’åˆ†æ•°æ®é›†
train_data = data_30[:train_size]
val_data = data_30[train_size:train_size + val_size]
test_data = data_30[train_size + val_size:]

print("æ•°æ®åˆ’åˆ†ç»“æœ:")
print(f"è®­ç»ƒé›†: {len(train_data)} ä¸ªæ ·æœ¬ ({TRAIN_RATIO*100:.1f}%)")
print(f"éªŒè¯é›†: {len(val_data)} ä¸ªæ ·æœ¬ ({VAL_RATIO*100:.1f}%)")
print(f"æµ‹è¯•é›†: {len(test_data)} ä¸ªæ ·æœ¬ ({TEST_RATIO*100:.1f}%)")

INPUT_DIM = 2560
OUTPUT_DIM = 256

# ç¦»æ•£æ—¶é—´æ™¶ä½“(DTC)å‚æ•°
N_QUBITS = 10               # é‡å­æ¯”ç‰¹æ•°
DTC_PERIOD = 8              # DTCå‘¨æœŸ
N_STEPS = 20                # æ¼”åŒ–æ­¥æ•°
DRIVE_STRENGTH = 0.8        # é©±åŠ¨å¼ºåº¦

IMG_QUBITS = int(np.ceil(np.log2(INPUT_DIM)))  # 12
COM_QUBITS = int(np.ceil(np.log2(OUTPUT_DIM)))  # 8
ALL_QUBITS = N_QUBITS  # 10ä¸ªé‡å­æ¯”ç‰¹ç”¨äºDTC

print(f"DTCé‡å­å‚¨å­˜åº“è®¡ç®—é…ç½®:")
print(f"  é‡å­æ¯”ç‰¹æ•°: {N_QUBITS}")
print(f"  DTCå‘¨æœŸ: {DTC_PERIOD}")
print(f"  æ¼”åŒ–æ­¥æ•°: {N_STEPS}")
print(f"  é©±åŠ¨å¼ºåº¦: {DRIVE_STRENGTH}")

# åˆå§‹åŒ–å¹¶ä¿å­˜ç»å…¸ç¥ç»ç½‘ç»œå‚æ•° - ä½¿ç”¨PyTorchå¼ é‡
WEIGHT = torch.randn(INPUT_DIM, OUTPUT_DIM, requires_grad=True) * 0.01
BIAS = torch.randn(1, OUTPUT_DIM, requires_grad=True)

# åˆ›å»ºä¿å­˜å‚æ•°çš„ç›®å½•
os.makedirs('model_parameters', exist_ok=True)

def save_initial_parameters():
    """ä¿å­˜åˆå§‹åŒ–çš„å‚æ•°"""
    torch.save(WEIGHT, 'model_parameters/initial_weight.pt')
    torch.save(BIAS, 'model_parameters/initial_bias.pt')
    print("Initial WEIGHT and BIAS saved!")

def sigmoid(x):
    return 1 / (1 + torch.exp(-x))

def normlize(x):
    norm = torch.norm(x)
    if norm == 0:
        return x
    return x / norm

def dense_layer(x):
    """ç»å…¸å¯†é›†å±‚ - æ•°æ®é¢„å¤„ç†å’Œå‹ç¼©"""
    if isinstance(x, np.ndarray):
        x = torch.from_numpy(x).float()
    output = torch.matmul(x, WEIGHT) + BIAS
    output = sigmoid(output)
    output = normlize(output[0])  # ç¡®ä¿è¾“å‡ºæ˜¯ä¸€ç»´çš„
    return output

# ============================================================
# ç¦»æ•£æ—¶é—´æ™¶ä½“(DTC)ç›¸å…³å‡½æ•°
# ============================================================

def dtc_drive_pulse(qubits, strength, duration=1.0):
    """
    åº”ç”¨DTCé©±åŠ¨è„‰å†² - å‘¨æœŸé©±åŠ¨äº§ç”Ÿæ—¶é—´æ™¶ä½“
    
    DTCçš„ç‰©ç†åŸºç¡€ï¼š
    H = J * Z_i * Z_{i+1} + Î© * X_i  (Ising + æ¨ªå‘åœº)
    å‘¨æœŸé©±åŠ¨ç ´åè¿ç»­å¯¹ç§°æ€§ï¼Œäº§ç”Ÿæ—¶é—´å¹³ç§»å¯¹ç§°æ€§ç ´ç¼º
    """
    for qubit in qubits:
        # Xæ–¹å‘çš„å…±æŒ¯é©±åŠ¨
        qml.RX(strength * duration, wires=qubit)
        # Zæ–¹å‘çš„è‡ªæ—‹ç›¸äº’ä½œç”¨
        qml.RZ(strength * 0.5 * duration, wires=qubit)

def dtc_ising_interaction(qubits, coupling_strength):
    """
    åº”ç”¨Isingç›¸äº’ä½œç”¨ - äº§ç”ŸDTCçš„å…³é”®
    H_Ising = J * Î£ Z_i * Z_{i+1}
    """
    n_qubits = len(qubits)
    for i in range(n_qubits - 1):
        qml.IsingZZ(coupling_strength, wires=[qubits[i], qubits[i+1]])
    # ç¯å½¢æ‹“æ‰‘
    qml.IsingZZ(coupling_strength, wires=[qubits[n_qubits-1], qubits[0]])

def dtc_evolution(qubits, drive_strength, num_steps):
    """
    ç¦»æ•£æ—¶é—´æ™¶ä½“æ¼”åŒ– - å‘¨æœŸé©±åŠ¨åŠ¨åŠ›å­¦
    
    ä¸€ä¸ªå‘¨æœŸåŒ…å«ï¼š
    1. Isingç›¸äº’ä½œç”¨
    2. é©±åŠ¨è„‰å†²
    
    é‡å¤num_stepsæ¬¡äº§ç”ŸDTCåŠ¨åŠ›å­¦
    """
    for step in range(num_steps):
        # Step 1: Isingç›¸äº’ä½œç”¨ (äº§ç”Ÿçº ç¼ )
        coupling = drive_strength * 0.3
        dtc_ising_interaction(qubits, coupling)
        
        # Step 2: é©±åŠ¨è„‰å†² (æ‰“ç ´è¿ç»­å¯¹ç§°æ€§)
        dtc_drive_pulse(qubits, drive_strength)
        
        # Step 3: é¢å¤–çš„å•æ¯”ç‰¹æ—‹è½¬ (å¢åŠ ä¿¡æ¯æ··åˆ)
        for qubit in qubits:
            phase = (drive_strength * step) % (2 * np.pi)
            qml.RY(phase * 0.1, wires=qubit)

def extract_dtc_features(qubits):
    """
    ä»DTCæ¼”åŒ–åçš„é‡å­æ€æå–ç‰¹å¾
    æµ‹é‡æ¯ä¸ªé‡å­æ¯”ç‰¹çš„Pauli ZæœŸæœ›å€¼
    """
    features = []
    for qubit in qubits:
        # æµ‹é‡ZæœŸæœ›å€¼
        features.append(qml.expval(qml.PauliZ(qubit)))
    return features

# å®šä¹‰é‡å­è®¾å¤‡å’Œç”µè·¯
dev = qml.device('lightning.qubit', wires=ALL_QUBITS)

@qml.qnode(dev, interface='torch')
def quantum_reservoir_circuit(img_params, drive_strength):
    '''
    åŸºäºç¦»æ•£æ—¶é—´æ™¶ä½“çš„é‡å­å‚¨å­˜åº“è®¡ç®—ç”µè·¯
    
    æ¶æ„:
    è¾“å…¥ç¼–ç  â†’ DTCæ¼”åŒ– â†’ ç‰¹å¾æå– â†’ æœŸæœ›å€¼æµ‹é‡
    
    ä¼˜åŠ¿:
    - æ¢¯åº¦è‡ªç”±è®¾è®¡ (å›ºå®šé©±åŠ¨å‚æ•°)
    - æ‹“æ‰‘å™ªå£°é²æ£’æ€§
    - ä¿¡æ¯è‡ªç„¶ç¼–ç åœ¨DTCåŠ¨åŠ›å­¦ä¸­
    '''
    
    # Step 1: å‚æ•°ç¼–ç å’Œé¢„å¤„ç†
    com_params = dense_layer(img_params)
    if len(com_params) < 2**COM_QUBITS:
        com_params_padded = torch.nn.functional.pad(com_params, (0, 2**COM_QUBITS - len(com_params)))
    else:
        com_params_padded = com_params[:2**COM_QUBITS]
    
    # Step 2: å¹…åº¦ç¼–ç  - å°†ç»å…¸æ•°æ®ç¼–ç åˆ°é‡å­æ€
    qml.AmplitudeEmbedding(com_params_padded, wires=range(COM_QUBITS), pad_with=0.0, normalize=True)
    
    # Step 3: åˆå§‹åŒ–å‰©ä½™é‡å­æ¯”ç‰¹
    for i in range(COM_QUBITS, ALL_QUBITS):
        qml.Hadamard(wires=i)
    
    # Step 4: DTCåŠ¨åŠ›å­¦æ¼”åŒ– - é‡å­å‚¨å­˜åº“
    # è¿™æ˜¯æ¢¯åº¦è‡ªç”±çš„ - é©±åŠ¨å‚æ•°æ˜¯å›ºå®šçš„
    dtc_evolution(range(ALL_QUBITS), drive_strength, N_STEPS)
    
    # Step 5: æå–ç‰¹å¾ - æµ‹é‡æ‰€æœ‰é‡å­æ¯”ç‰¹çš„Pauli ZæœŸæœ›å€¼
    features = extract_dtc_features(range(ALL_QUBITS))
    
    # è¿”å›ç‰¹å¾çš„å’Œä½œä¸ºè¾“å‡º
    return sum(features)

@qml.qnode(dev, interface='torch')
def quantum_reservoir_circuit_with_readout(img_params, drive_strength):
    '''
    é‡å­å‚¨å­˜åº“ + çº¿æ€§è¯»å–å¤´
    ç»å…¸çº¿æ€§åˆ†ç±»å™¨å¯¹æå–çš„ç‰¹å¾è¿›è¡Œå¤„ç†
    '''
    
    # Step 1-4: åŒä¸Š
    com_params = dense_layer(img_params)
    if len(com_params) < 2**COM_QUBITS:
        com_params_padded = torch.nn.functional.pad(com_params, (0, 2**COM_QUBITS - len(com_params)))
    else:
        com_params_padded = com_params[:2**COM_QUBITS]
    
    qml.AmplitudeEmbedding(com_params_padded, wires=range(COM_QUBITS), pad_with=0.0, normalize=True)
    
    for i in range(COM_QUBITS, ALL_QUBITS):
        qml.Hadamard(wires=i)
    
    dtc_evolution(range(ALL_QUBITS), drive_strength, N_STEPS)
    
    # è¿”å›æ‰€æœ‰é‡å­æ¯”ç‰¹çš„ZæœŸæœ›å€¼ä½œä¸ºç‰¹å¾å‘é‡
    return [qml.expval(qml.PauliZ(i)) for i in range(ALL_QUBITS)]

class LinearReadout(nn.Module):
    """
    ç»å…¸çº¿æ€§è¯»å–å¤´ - å¯¹DTCæå–çš„ç‰¹å¾è¿›è¡Œåˆ†ç±»
    è¿™æ˜¯å”¯ä¸€éœ€è¦è®­ç»ƒçš„éƒ¨åˆ†
    """
    def __init__(self, input_size, output_size=1):
        super(LinearReadout, self).__init__()
        self.linear = nn.Linear(input_size, output_size)
        
    def forward(self, features):
        return self.linear(features)

# æ‰¹é‡å¤„ç†å‡½æ•° - DTCç‰ˆæœ¬
def process_batch_dtc(img_batch, drive_strength, readout=None):
    '''å¤„ç†æ‰¹é‡æ ·æœ¬ - é‡å­å‚¨å­˜åº“ç‰ˆæœ¬'''
    batch_results = []
    
    for img_params in img_batch:
        # ç¡®ä¿è¾“å…¥æ˜¯PyTorchå¼ é‡
        if isinstance(img_params, np.ndarray):
            img_params = torch.from_numpy(img_params).float()
        
        # è¿è¡ŒDTCç”µè·¯è·å¾—ç‰¹å¾
        features = quantum_reservoir_circuit_with_readout(img_params, drive_strength)
        
        # è½¬æ¢ä¸ºå¼ é‡
        features_tensor = torch.tensor(features, dtype=torch.float32)
        
        # é€šè¿‡çº¿æ€§è¯»å–å¤´
        if readout is not None:
            output = readout(features_tensor)
        else:
            output = torch.sum(features_tensor)
        
        # ç¡®ä¿ç»“æœæ˜¯å®æ•°
        if isinstance(output, (complex, np.complex128)):
            output = torch.tensor(np.real(output), dtype=torch.float32)
        
        batch_results.append(output)
    
    return torch.stack(batch_results)

def validate_model_dtc(drive_strength, readout, val_samples=1000):
    """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ - DTCç‰ˆæœ¬"""
    try:
        val_subset = val_data[:min(val_samples, len(val_data))]
        results = process_batch_dtc(val_subset, drive_strength, readout)
        return float(torch.mean(results))
    except Exception as e:
        print(f"Validation error: {e}")
        return float('inf')

# æ‰¹é‡è®­ç»ƒå‡½æ•° - DTCç‰ˆæœ¬
def train_batch_version_dtc():
    try:
        # ä¿å­˜åˆå§‹å‚æ•°
        save_initial_parameters()
        
        # ä½¿ç”¨è®­ç»ƒé›†
        n_samples = 1000
        samples = train_data[:n_samples]

        # åˆå§‹åŒ–çº¿æ€§è¯»å–å¤´
        readout = LinearReadout(ALL_QUBITS, output_size=1)
        
        # ä¼˜åŒ–å™¨ - ä»…ä¼˜åŒ–è¯»å–å¤´çš„æƒé‡ (DTCé©±åŠ¨å‚æ•°å›ºå®š)
        opt = torch.optim.SGD(readout.parameters(), lr=0.01)
        
        # ä¿å­˜åˆå§‹é©±åŠ¨å‚æ•°
        initial_config = {
            'drive_strength': DRIVE_STRENGTH,
            'n_steps': N_STEPS,
            'n_qubits': ALL_QUBITS,
            'dtc_period': DTC_PERIOD
        }
        torch.save(initial_config, 'model_parameters/initial_dtc_config.pt')
        torch.save(readout.state_dict(), 'model_parameters/initial_readout_weights.pt')
        print("Initial DTC configuration and readout weights saved!")
        print(f"  Drive strength: {DRIVE_STRENGTH}")
        print(f"  Evolution steps: {N_STEPS}")
        print(f"  Readout parameters: {sum(p.numel() for p in readout.parameters())}")

        n_epochs = 5
        batch_size = 50
        
        # è®°å½•è®­ç»ƒå†å²
        training_history = {
            'epoch_losses': [],
            'val_losses': [],
            'batch_losses': [],
            'readout_weights_history': [],
            'dtc_architecture': {
                'type': 'QuantumReservoirComputing_DTC',
                'algorithm': 'DiscreteTimeCrystal',
                'n_qubits': ALL_QUBITS,
                'evolution_steps': N_STEPS,
                'dtc_period': DTC_PERIOD,
                'drive_strength': DRIVE_STRENGTH,
                'gradient_free': True,
                'topological_noise_robustness': True,
                'readout_type': 'LinearReadout'
            },
            'data_split_info': {
                'train_size': len(train_data),
                'val_size': len(val_data),
                'test_size': len(test_data),
                'actual_train_used': n_samples
            }
        }

        print("\nStarting Quantum Reservoir Computing with DTC training...")
        print("=" * 60)
        print("Note: DTC reservoir is fixed, only linear readout is trained")
        print("=" * 60)
        start_time = time.time()

        for epoch in range(n_epochs):
            epoch_loss = 0.0
            batch_count = 0

            for i in range(0, n_samples, batch_size):
                batch = samples[i:i+batch_size]
                
                def closure():
                    opt.zero_grad()
                    results = process_batch_dtc(batch, DRIVE_STRENGTH, readout)
                    loss = torch.mean(results)
                    loss.backward()
                    return loss
                
                # è®°å½•è®­ç»ƒå‰çš„æƒé‡
                pre_readout_norm = torch.norm(torch.cat([p.flatten() for p in readout.parameters()]))
                
                # æ›´æ–°æƒé‡
                loss = opt.step(closure)
                current_loss = loss.item() if hasattr(loss, 'item') else float(loss)
                epoch_loss += current_loss
                batch_count += 1

                # è®°å½•è®­ç»ƒåçš„æƒé‡
                post_readout_norm = torch.norm(torch.cat([p.flatten() for p in readout.parameters()]))
                
                # è®°å½•æ‰¹æ¬¡ä¿¡æ¯
                training_history['batch_losses'].append({
                    'epoch': epoch,
                    'batch': i // batch_size,
                    'loss': float(current_loss),
                    'pre_readout_norm': float(pre_readout_norm),
                    'post_readout_norm': float(post_readout_norm),
                    'drive_strength': DRIVE_STRENGTH
                })

                if (i // batch_size) % 5 == 0:
                    print(f"Epoch {epoch}, Batch {i//batch_size}: loss = {current_loss:.6f}")

            if batch_count > 0:
                avg_epoch_loss = epoch_loss / batch_count
                # è®¡ç®—éªŒè¯æŸå¤±
                val_loss = validate_model_dtc(DRIVE_STRENGTH, readout, val_samples=500)
                
                training_history['epoch_losses'].append({
                    'epoch': epoch,
                    'avg_loss': float(avg_epoch_loss)
                })
                training_history['val_losses'].append({
                    'epoch': epoch,
                    'val_loss': float(val_loss)
                })
                
                # ä¿å­˜æ¯ä¸ªepochçš„è¯»å–å¤´æƒé‡
                epoch_readout_state = {
                    'weight': readout.linear.weight.clone().detach(),
                    'bias': readout.linear.bias.clone().detach()
                }
                training_history['readout_weights_history'].append(epoch_readout_state)
                torch.save(epoch_readout_state, f'model_parameters/readout_weights_epoch_{epoch}.pt')
                
                print(f"Epoch {epoch} completed: Train Loss = {avg_epoch_loss:.6f}, Val Loss = {val_loss:.6f}")
                print(f"  Readout weight norm: {torch.norm(readout.linear.weight):.6f}")
                print(f"Readout weights for epoch {epoch} saved!")
                print("-" * 60)

        total_time = time.time() - start_time
        print(f"Training completed in {total_time:.2f} seconds!")
        
        # ä¿å­˜æœ€ç»ˆæƒé‡å’Œè®­ç»ƒå†å²
        final_readout_state = {
            'weight': readout.linear.weight.clone().detach(),
            'bias': readout.linear.bias.clone().detach()
        }
        
        torch.save(final_readout_state, 'model_parameters/final_readout_weights.pt')
        torch.save(readout.state_dict(), 'model_parameters/final_readout_model.pt')
        torch.save(training_history, 'model_parameters/training_history.pt')
        print("Final readout weights and training history saved!")
        
        return readout, training_history

    except Exception as e:
        print(f"Error in DTC training: {e}")
        import traceback
        traceback.print_exc()
        return None, None


# æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
def test_trained_model_dtc(readout, test_samples=1000):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹ - DTCç‰ˆæœ¬"""
    print("\nTesting trained DTC quantum reservoir on test set...")
    try:
        test_subset = test_data[:min(test_samples, len(test_data))]
        results = process_batch_dtc(test_subset, DRIVE_STRENGTH, readout)
        print(f"Test results on {len(test_subset)} samples:")
        for i in range(min(5, len(results))):
            print(f"  Sample {i}: {results[i].item():.6f}")
        if len(results) > 5:
            print(f"  ... (showing first 5 of {len(results)} results)")
        avg_result = torch.mean(results).item()
        std_result = torch.std(results).item()
        print(f"Average test result: {avg_result:.6f}")
        print(f"Standard deviation: {std_result:.6f}")
        print(f"Min: {torch.min(results).item():.6f}, Max: {torch.max(results).item():.6f}")
        return results
    except Exception as e:
        print(f"Error in testing: {e}")
        return None


# ä¸»ç¨‹åº
if __name__ == "__main__":
    print("Starting Quantum Reservoir Computing with Discrete Time Crystal...")
    print("=" * 60)
    print(f"Data Split: {TRAIN_RATIO*100:.0f}% Train, {VAL_RATIO*100:.0f}% Validation, {TEST_RATIO*100:.0f}% Test")
    print(f"Training set: {len(train_data)} samples")
    print(f"Validation set: {len(val_data)} samples")
    print(f"Test set: {len(test_data)} samples")
    print("=" * 60)
    print("\nğŸ”¬ Physical Principles:")
    print("  â€¢ DTC exploits periodic driving to break continuous symmetry")
    print("  â€¢ Quantum state encodes information through time crystal dynamics")
    print("  â€¢ Linear readout extracts features from DTC evolution")
    print("  â€¢ Gradient-free training: only readout is optimized")
    print("  â€¢ Topological noise robustness from DTC properties")
    print("=" * 60)
    
    # è®­ç»ƒDTCé‡å­å‚¨å­˜åº“
    readout_model, history = train_batch_version_dtc()
    
    if readout_model is not None:
        # æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
        test_results = test_trained_model_dtc(readout_model)
        
        # æ˜¾ç¤ºè®­ç»ƒæ€»ç»“
        print("\n" + "=" * 60)
        print("QUANTUM RESERVOIR COMPUTING WITH DTC - TRAINING SUMMARY:")
        print("=" * 60)
        print(f"Data split: {TRAIN_RATIO*100:.1f}% train, {VAL_RATIO*100:.1f}% val, {TEST_RATIO*100:.1f}% test")
        print(f"Training samples used: {history['data_split_info']['actual_train_used']}")
        print(f"Total training samples available: {len(train_data)}")
        print(f"Validation samples: {len(val_data)}")
        print(f"Test samples: {len(test_data)}")
        print("\nClassical NN parameters (preprocessing):")
        print(f"  - WEIGHT shape: {WEIGHT.shape}")
        print(f"  - BIAS shape: {BIAS.shape}")
        print("\nQuantum Reservoir Architecture (DTC):")
        arch = history['dtc_architecture']
        print(f"  - Algorithm: {arch['algorithm']}")
        print(f"  - Quantum qubits: {arch['n_qubits']}")
        print(f"  - Evolution steps: {arch['evolution_steps']}")
        print(f"  - DTC period: {arch['dtc_period']}")
        print(f"  - Drive strength: {arch['drive_strength']}")
        print(f"  - Gradient-free: {arch['gradient_free']}")
        print(f"  - Topological noise robustness: {arch['topological_noise_robustness']}")
        print(f"  - Readout type: {arch['readout_type']}")
        print("\nTrainable readout parameters:")
        for name, param in readout_model.named_parameters():
            print(f"  - {name}: {param.shape}")
        print(f"  - Total parameters: {sum(p.numel() for p in readout_model.parameters())}")
        print(f"\nTraining epochs: {len(history['epoch_losses'])}")
        if len(history['epoch_losses']) > 0:
            print(f"  - Final train loss: {history['epoch_losses'][-1]['avg_loss']:.6f}")
        if len(history['val_losses']) > 0:
            print(f"  - Final validation loss: {history['val_losses'][-1]['val_loss']:.6f}")
        
        # æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶
        print("\nSaved files in 'model_parameters' directory:")
        saved_files = os.listdir('model_parameters')
        for file in sorted(saved_files):
            print(f"  - {file}")
        
        print("\nğŸ’¡ Key Advantages over Standard Parametrized Quantum Circuits:")
        print("  âœ“ Gradient-free training avoids barren plateaus")
        print("  âœ“ Fixed DTC dynamics reduce optimization complexity")
        print("  âœ“ Topological protection provides noise robustness")
        print("  âœ“ Natural information encoding through time crystal dynamics")
        print("  âœ“ Fewer trainable parameters (only readout)")
        print("  âœ“ Suitable for NISQ devices")
    else:
        print("Training failed!")
